{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generator using LSTM and Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Generator is basically to use Deep Learning to develop a language model to generate new pieces of text by training on a corpus of data and let the model emit new word sequences given a seed word.  \n",
    "In this section we will make using LSTM and Keras to train a model on collection of William Shakespeare's sonnects that can downloaded online and then make use of the model to make predictions.\n",
    "The entire modle was trained on a Ubuntu Machine with 1 Nvidia Tesla K80 GPU. It took me around 2 hours to train for 20 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinudaniel74/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RNN\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data can be downloaded from the [project gutenberg](http://www.gutenberg.org/ebooks/1041?msg=welcome_stranger). I cleaned up this file to remove the start and end credits, and it can be downloaded from my git repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'sonnet.txt'\n",
    "text = open(filename).read()\n",
    "text = text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Character Mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charater Mapping is a step in which we assign an arbitrary number to a character in the text. In this way, all unique characters are mapped to a number. This is important, because machines understand numbers far better than text, and this subsequently makes the training process easier.\n",
    "\n",
    "We can also make use of word mappings where we assign a number to a word instead of a character but since this is a small data set going with Character Mappings makes sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = sorted(list(set(text)))\n",
    "\n",
    "n_to_char = {n:char for n, char in enumerate(characters)}\n",
    "char_to_n = {char:n for n, char in enumerate(characters)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " \"'\": 3,\n",
       " '(': 4,\n",
       " ')': 5,\n",
       " ',': 6,\n",
       " '-': 7,\n",
       " '.': 8,\n",
       " ':': 9,\n",
       " ';': 10,\n",
       " '?': 11,\n",
       " 'a': 12,\n",
       " 'b': 13,\n",
       " 'c': 14,\n",
       " 'd': 15,\n",
       " 'e': 16,\n",
       " 'f': 17,\n",
       " 'g': 18,\n",
       " 'h': 19,\n",
       " 'i': 20,\n",
       " 'j': 21,\n",
       " 'k': 22,\n",
       " 'l': 23,\n",
       " 'm': 24,\n",
       " 'n': 25,\n",
       " 'o': 26,\n",
       " 'p': 27,\n",
       " 'q': 28,\n",
       " 'r': 29,\n",
       " 's': 30,\n",
       " 't': 31,\n",
       " 'u': 32,\n",
       " 'v': 33,\n",
       " 'w': 34,\n",
       " 'x': 35,\n",
       " 'y': 36,\n",
       " 'z': 37}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  100229\n",
      "Total Vocab:  38\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(text)\n",
    "n_vocab = len(characters)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the most tricky part when it comes to LSTM. seq_length is the sequence of characters that we want to consider before making a prediction. In our case it will be 100.\n",
    "So for the sake of simplicity let's assume that we have a seq_length of 4 and if our entire corpus contains only the word \"Machine\" the X and Y array will be as follows\n",
    "\n",
    "| X  | Y |\n",
    "| ------------- | ------------- |\n",
    "| `[M,a,c,h]`  | `[i]`  |\n",
    "| `[a,c,h,i]` | `[n]`  |\n",
    "| `[c,h,i,n]` | `[e]`  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  100129\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "X = []\n",
    "Y = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = text[i:i + seq_length]\n",
    "    seq_out = text[i + seq_length]\n",
    "    X.append([char_to_n[char] for char in seq_in])\n",
    "    Y.append(char_to_n[seq_out])\n",
    "\n",
    "print(\"Total Patterns: \", len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must transform the list of input sequences into the form `[samples, time steps, features]` expected by an LSTM network. Next we need to rescale the integers to the range 0-1 for the LSTM network to learn faster. Alos let us convert the outputs to a one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_modified = np.reshape(X, (len(X), seq_length, 1))\n",
    "X_modified = X_modified / float(n_vocab)\n",
    "Y_modified = np_utils.to_categorical(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a sequential model using LSTM. We will make use of 2 LSTM layer's with 400 units and add a dropout of 20% to avoid overfitting. In order for the next LSTM layer to be able to process the same sequences, we enter the return_sequences parameter as True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(400, input_shape=(X_modified.shape[1], X_modified.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(400))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(Y_modified.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is slow to train (around 300 sec for 1 epoch). we will use model checkpointing to record all of the network weights to file each time an improvement in loss is observed at the end of the epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "100129/100129 [==============================] - 321s 3ms/step - loss: 2.9067\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.90667, saving model to weights-improvement-01-2.9067.hdf5\n",
      "Epoch 2/20\n",
      "100129/100129 [==============================] - 303s 3ms/step - loss: 2.5285\n",
      "\n",
      "Epoch 00002: loss improved from 2.90667 to 2.52853, saving model to weights-improvement-02-2.5285.hdf5\n",
      "Epoch 3/20\n",
      "100129/100129 [==============================] - 303s 3ms/step - loss: 2.3772\n",
      "\n",
      "Epoch 00003: loss improved from 2.52853 to 2.37719, saving model to weights-improvement-03-2.3772.hdf5\n",
      "Epoch 4/20\n",
      "100129/100129 [==============================] - 302s 3ms/step - loss: 2.2700\n",
      "\n",
      "Epoch 00004: loss improved from 2.37719 to 2.27001, saving model to weights-improvement-04-2.2700.hdf5\n",
      "Epoch 5/20\n",
      "100129/100129 [==============================] - 303s 3ms/step - loss: 2.1675\n",
      "\n",
      "Epoch 00005: loss improved from 2.27001 to 2.16752, saving model to weights-improvement-05-2.1675.hdf5\n",
      "Epoch 6/20\n",
      "100129/100129 [==============================] - 302s 3ms/step - loss: 2.0687\n",
      "\n",
      "Epoch 00006: loss improved from 2.16752 to 2.06866, saving model to weights-improvement-06-2.0687.hdf5\n",
      "Epoch 7/20\n",
      "100129/100129 [==============================] - 302s 3ms/step - loss: 1.9835\n",
      "\n",
      "Epoch 00007: loss improved from 2.06866 to 1.98353, saving model to weights-improvement-07-1.9835.hdf5\n",
      "Epoch 8/20\n",
      "100129/100129 [==============================] - 303s 3ms/step - loss: 1.9136\n",
      "\n",
      "Epoch 00008: loss improved from 1.98353 to 1.91365, saving model to weights-improvement-08-1.9136.hdf5\n",
      "Epoch 9/20\n",
      "100129/100129 [==============================] - 302s 3ms/step - loss: 1.8475\n",
      "\n",
      "Epoch 00009: loss improved from 1.91365 to 1.84750, saving model to weights-improvement-09-1.8475.hdf5\n",
      "Epoch 10/20\n",
      "100129/100129 [==============================] - 303s 3ms/step - loss: 1.7854\n",
      "\n",
      "Epoch 00010: loss improved from 1.84750 to 1.78538, saving model to weights-improvement-10-1.7854.hdf5\n",
      "Epoch 11/20\n",
      "100129/100129 [==============================] - 302s 3ms/step - loss: 1.7271\n",
      "\n",
      "Epoch 00011: loss improved from 1.78538 to 1.72711, saving model to weights-improvement-11-1.7271.hdf5\n",
      "Epoch 12/20\n",
      "100129/100129 [==============================] - 302s 3ms/step - loss: 1.6659\n",
      "\n",
      "Epoch 00012: loss improved from 1.72711 to 1.66591, saving model to weights-improvement-12-1.6659.hdf5\n",
      "Epoch 13/20\n",
      "100129/100129 [==============================] - 303s 3ms/step - loss: 1.6158\n",
      "\n",
      "Epoch 00013: loss improved from 1.66591 to 1.61582, saving model to weights-improvement-13-1.6158.hdf5\n",
      "Epoch 14/20\n",
      "100129/100129 [==============================] - 302s 3ms/step - loss: 1.5606\n",
      "\n",
      "Epoch 00014: loss improved from 1.61582 to 1.56058, saving model to weights-improvement-14-1.5606.hdf5\n",
      "Epoch 15/20\n",
      "100129/100129 [==============================] - 301s 3ms/step - loss: 1.5042\n",
      "\n",
      "Epoch 00015: loss improved from 1.56058 to 1.50420, saving model to weights-improvement-15-1.5042.hdf5\n",
      "Epoch 16/20\n",
      "100129/100129 [==============================] - 303s 3ms/step - loss: 1.4521\n",
      "\n",
      "Epoch 00016: loss improved from 1.50420 to 1.45206, saving model to weights-improvement-16-1.4521.hdf5\n",
      "Epoch 17/20\n",
      "100129/100129 [==============================] - 303s 3ms/step - loss: 1.4000\n",
      "\n",
      "Epoch 00017: loss improved from 1.45206 to 1.40005, saving model to weights-improvement-17-1.4000.hdf5\n",
      "Epoch 18/20\n",
      "100129/100129 [==============================] - 303s 3ms/step - loss: 1.3527\n",
      "\n",
      "Epoch 00018: loss improved from 1.40005 to 1.35269, saving model to weights-improvement-18-1.3527.hdf5\n",
      "Epoch 19/20\n",
      "100129/100129 [==============================] - 303s 3ms/step - loss: 1.3069\n",
      "\n",
      "Epoch 00019: loss improved from 1.35269 to 1.30694, saving model to weights-improvement-19-1.3069.hdf5\n",
      "Epoch 20/20\n",
      "100129/100129 [==============================] - 302s 3ms/step - loss: 1.2610\n",
      "\n",
      "Epoch 00020: loss improved from 1.30694 to 1.26103, saving model to weights-improvement-20-1.2610.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f977719bd68>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_modified, Y_modified, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"weights-improvement-20-1.2610.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start making some predictions. We will start of with some random sequence of 100 characters from our Training set and given this random seed let's predict another 1000 charcters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" stopped are.\n",
      "  mark how with my neglect i do dispense:\n",
      "    you are so strongly in my purpose bred,\n",
      "  \"\n",
      "   the hard the beauty of your srue inace,\n",
      "    and there bur shou, that i am sometime thee,\n",
      "    and there bur shou, that words the lovnne oray.\n",
      "    and there bur srieer than thou dost stain,\n",
      "    and there thou thalt wour should mote be to done.\n",
      "\n",
      "  lxxxiii\n",
      "\n",
      "  when i have seen the world should would have seen to come,\n",
      "  since what is hn the surength of all the sime,\n",
      "  and seamo summer's fear will be thy faces,\n",
      "  and see that love with thee i see surange;\n",
      "  the some will better that thou dost sece stre,\n",
      "  and therefore lake the wirhow will of youth,\n",
      "  and therefore lake the wirhow will of youth,\n",
      "  and therefore lake the wirhow will of youth,\n",
      "  and therefore lake the wirhow will of youth,\n",
      "  and therefore lake the wirhow will of youth,\n",
      "  and therefore lake the wirhow will of youth,\n",
      "  and therefore lake the wirhow will of youth,\n",
      "  and therefore lake the wirhow will of youth,\n",
      "  and therefore lake the wirhow will of youth,\n",
      "  and therefore lake the wirhow will of youth,\n",
      "  and therefore lake the\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = np.random.randint(0, len(X)-1)\n",
    "pattern = X[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([n_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = n_to_char[index]\n",
    "    seq_in = [n_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From what we can see from the predicted text it has observed the patterns in the data. It also knows some the words but some of the words have lost their meaning. It has alos understood where to add the punctuation marks which is pretty suprising.\n",
    "We can improve the model performance by  training a deeper network (may be adding a another LSTM layer or increasing the LSTM units in each layer) or we can train for more number of epochs.\n",
    "\n",
    "I would recommend reading Andrej Karpthy's [The UnReasonable Effectiveness of RNN](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) whihc delves deeper on this subject and gives some eye popping results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
